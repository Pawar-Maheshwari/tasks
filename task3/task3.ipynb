{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOl1jwipqGCpIEoj/98a7aq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install opencv-python opencv-contrib-python numpy pandas keras tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xaFUaGTGQ6iJ","executionInfo":{"status":"ok","timestamp":1744398060128,"user_tz":-330,"elapsed":5984,"user":{"displayName":"Akshada Gunjal","userId":"17152451300241560849"}},"outputId":"59d998f8-019a-4878-adeb-4a27d95af743"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from keras.models import load_model\n","from datetime import datetime\n","import pandas as pd\n","import os\n","from time import sleep\n","\n","# Load Emotion Detection Model\n","emotion_model = load_model(\"fer2013_mini_XCEPTION.102-0.66.hdf5\", compile=False)\n","emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n","\n","# Load Face Detection and Recognition Models\n","face_detector = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n","face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n","\n","# --------------------- Register & Train ---------------------\n","def capture_samples(student_name, num_samples=5):\n","    os.makedirs(f\"student_images/{student_name}\", exist_ok=True)\n","    cap = cv2.VideoCapture(0)\n","    count = 0\n","    while count < num_samples:\n","        ret, frame = cap.read()\n","        if not ret:\n","            continue\n","        (h, w) = frame.shape[:2]\n","        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n","                                     (300, 300), (104.0, 177.0, 123.0))\n","        face_detector.setInput(blob)\n","        detections = face_detector.forward()\n","\n","        for i in range(detections.shape[2]):\n","            confidence = detections[0, 0, i, 2]\n","            if confidence > 0.7:\n","                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","                (x1, y1, x2, y2) = box.astype(\"int\")\n","                face = frame[y1:y2, x1:x2]\n","                if face.size > 0:\n","                    gray_face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n","                    cv2.imwrite(f\"student_images/{student_name}/sample_{count}.jpg\", gray_face)\n","                    count += 1\n","                    sleep(1)\n","        cv2.imshow(\"Capturing Samples\", frame)\n","        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n","            break\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","def train_recognizer():\n","    faces, ids = [], []\n","    label_ids = {}\n","    current_id = 0\n","\n","    for root, dirs, files in os.walk(\"student_images\"):\n","        for file in files:\n","            if file.endswith((\"jpg\", \"png\")):\n","                path = os.path.join(root, file)\n","                label = os.path.basename(root)\n","                if label not in label_ids:\n","                    label_ids[label] = current_id\n","                    current_id += 1\n","                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","                if img is not None:\n","                    faces.append(img)\n","                    ids.append(label_ids[label])\n","\n","    if faces:\n","        face_recognizer.train(faces, np.array(ids))\n","    return label_ids\n","\n","label_ids = train_recognizer()\n","id_to_name = {v: k for k, v in label_ids.items()}\n","\n","# --------------------- Emotion Detection ---------------------\n","def detect_emotion(face_img):\n","    try:\n","        face_img = cv2.resize(face_img, (64, 64))\n","        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n","        face_img = np.reshape(face_img, [1, 64, 64, 1]).astype('float32') / 255.0\n","        prediction = emotion_model.predict(face_img)\n","        return emotion_labels[np.argmax(prediction)]\n","    except:\n","        return \"Unknown\"\n","\n","# --------------------- Take Attendance ---------------------\n","def take_attendance():\n","    attendance = {name: {'status': 'Absent', 'emotion': 'Unknown', 'time': ''} for name in id_to_name.values()}\n","    cap = cv2.VideoCapture(0)\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            continue\n","\n","        (h, w) = frame.shape[:2]\n","        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n","                                     (300, 300), (104.0, 177.0, 123.0))\n","        face_detector.setInput(blob)\n","        detections = face_detector.forward()\n","\n","        for i in range(detections.shape[2]):\n","            confidence = detections[0, 0, i, 2]\n","            if confidence > 0.7:\n","                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","                (x1, y1, x2, y2) = box.astype(\"int\")\n","                face = frame[y1:y2, x1:x2]\n","                if face.size > 0:\n","                    emotion = detect_emotion(face)\n","                    gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n","                    try:\n","                        id_, conf = face_recognizer.predict(gray)\n","                        if conf < 100:\n","                            name = id_to_name[id_]\n","                            attendance[name] = {\n","                                'status': 'Present',\n","                                'emotion': emotion,\n","                                'time': datetime.now().strftime(\"%H:%M:%S\")\n","                            }\n","                            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","                            cv2.putText(frame, f\"{name} - {emotion}\", (x1, y1 - 10),\n","                                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n","                    except:\n","                        pass\n","\n","        cv2.imshow(\"Attendance System\", frame)\n","        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n","            break\n","\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","    # Save to CSV\n","    df = pd.DataFrame.from_dict(attendance, orient='index').reset_index()\n","    df.columns = ['Student', 'Status', 'Emotion', 'Time']\n","    df['Date'] = datetime.now().strftime(\"%Y-%m-%d\")\n","    os.makedirs(\"attendance_reports\", exist_ok=True)\n","    df.to_csv(f'attendance_reports/{df[\"Date\"][0]}.csv', index=False)\n","    print(f\"âœ… Attendance saved to attendance_reports/{df['Date'][0]}.csv\")\n","\n","# --------------------- Usage ---------------------\n","# ðŸ‘‰ Register a student:\n","#register_student(\"student_name\")\n","\n","# ðŸ‘‰ Start attendance:\n","# take_attendance()\n"],"metadata":{"id":"CoSDrYgSRAIe","executionInfo":{"status":"ok","timestamp":1744399219163,"user_tz":-330,"elapsed":590,"user":{"displayName":"Akshada Gunjal","userId":"17152451300241560849"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["capture_samples(\"student1\")\n","capture_samples(\"student2\")\n","label_ids = train_recognizer()\n","id_to_name = {v: k for k, v in label_ids.items()}\n"],"metadata":{"id":"J690OSZWNX-J","colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"status":"error","timestamp":1744399532983,"user_tz":-330,"elapsed":307398,"user":{"displayName":"Akshada Gunjal","userId":"17152451300241560849"}},"outputId":"c8e0129b-4286-4dfa-afb2-c8578c2d4ae7"},"execution_count":22,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-62685add4b03>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcapture_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"student1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcapture_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"student2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_recognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mid_to_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-464bcf6e70d4>\u001b[0m in \u001b[0;36mcapture_samples\u001b[0;34m(student_name, num_samples)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}